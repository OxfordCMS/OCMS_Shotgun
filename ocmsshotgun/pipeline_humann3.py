"""
===========================
pipeline_humann3.py
===========================

Overview
========

This pipeline uses humann3 to take raw fastq files and estimate 
taxonomic abundunce, then functionally profile the metagenome.
files :file:``pipeline.yml` and :file:`conf.py`.

Usage
=====

See :ref:`PipelineSettingUp` and :ref:`PipelineRunning` on general
information how to use cgat pipelines.

Configuration
-------------

The pipeline requires a configured :file:`pipeline.yml` file.

Default configuration files can be generated by executing:

   python <srcdir>/pipeline_humann.py config

Input files
-----------

Input files should be fastq.gz
Paired-end reads should be concatenated into one fastq.gz file. 
Can use concat_fastq.py for this -- paired end fastq files are 
in the format .fastq.1.gz and .fastq.2.gz.

Requirements
------------


Dependencies
============
Bowtie2
DIAMOND
R
Pandoc

module load Bowtie2/2.4.1-GCC-9.3.0
module load DIAMOND/0.9.36-GCC-9.3.0
module load R/4.0.0-foss-2020a
module load R-bundle-Bioconductor/3.11-foss-2020a-R-4.0.0
module load Pandoc/2.13

Pipeline output
===============


Glossary
========

.. glossary::


Code
====

"""
import sys
import os
import glob
import re
import shutil
from ruffus import *
from cgatcore import pipeline as P

import ocmsshotgun.modules.Utility as utility
import ocmsshotgun.modules.Humann3 as H

# load options from the config file
PARAMS = P.get_parameters(
    ["pipeline.yml"])

# check all files to be processed
FASTQ1s = utility.check_input(PARAMS.get('location_input','.'))

if PARAMS['location_transcriptome']:
    FASTQ2s = utility.check_input(PARAMS['location_transcriptome'])
else:
    FASTQ2s = None

###############################################################################
# Run humann3 on concatenated fastq.gz
###############################################################################
@follows(mkdir("input_merged.dir"))
@transform(FASTQ1s,
           regex(".+/(.+).fastq.1.gz"),
           r"input_merged.dir/\1.fastq.gz")
def poolInputFastqs(infile, outfile):
    '''Humann relies on pooling input files'''

    infiles = utility.matchReference(infile, outfile, **PARAMS)
    fastqs = [i for i in [infiles.fastq1, infiles.fastq2, infiles.fastq3] if i]
    
    if len(fastqs) == 1:
        utility.symlink(infile, outfile)
    else:
        fastqs = ' '.join(fastqs)
        statement = "cat %(fastqs)s > %(outfile)s"
        P.run(statement)
    
###############################################################################
# Run humann3 on concatenated fastq.gz
# produces a humann3.dir with a folder for each sample, which contains 
# pathcoverage, pathabundance and genefamilies files.
###############################################################################
@follows(mkdir("humann3.dir"))
@subdivide(poolInputFastqs,
           regex(".+/(.+).fastq.gz"),
           r"humann3.dir/\1/\1_*.tsv.gz")
def runHumann3(infile, outfiles):
    '''functional profile with humann3'''
    
    outfile = P.snip(infile, '.fastq.gz', strip_path=True)
    outfile = os.path.join('humann3.dir',
                            outfile,
                            outfile + '_pathcoverage.tsv.gz')
    
    tool = H.humann3(infile, outfile, **PARAMS)
    statement = ' && '.join([tool.buildStatement(), tool.postProcess()])
        
    P.run(statement,
          job_memory = PARAMS["humann3_job_memory"],
          job_threads = PARAMS["humann3_job_threads"],
          job_options = PARAMS.get('humann3_job_options',''))

    
###############################################################################
# Run humann3 on metatranscriptome data
###############################################################################
@active_if(PARAMS['location_transcriptome'])    
@transform(FASTQ2s,
           regex(".+/(.+).fastq.1.gz"),
           r"input_mtx_merged.dir/\1.fastq.gz")
def poolTranscriptomeFastqs(infile, outfile):
    '''Humann relies on pooling input files'''
    
    # make output directory
    outdir = os.path.abspath(os.path.dirname(outfile))
    if os.path.exists(outdir):
        shutil.rmtree(outdir)
    os.mkdir(outdir)

    infiles = utility.matchReference(infile, outfile, **PARAMS)
    fastqs = [i for i in [infiles.fastq1, infiles.fastq2, infiles.fastq3] if i]

    if len(fastqs) == 1:
        utility.symlink(infile, outfile)
    else:
        fastqs = ' '.join(fastqs)
        statement = "cat %(fastqs)s > %(outfile)s"
        P.run(statement)

@follows(runHumann3)
@subdivide(poolTranscriptomeFastqs,
           regex(".+/(.+).fastq.gz"),
           add_inputs(r"humann3.dir/\1/\1_metaphlan_bugs_list.tsv.gz"),
           r"humann3_mtx.dir/\1/\1_*.tsv.gz")
def runHumann3_metatranscriptome(infiles, outfiles):
    '''Optionally run humann3 on matched metatranscriptome data, making 
    use of the metaphlan_bugs_list file output from the metagenome analysis'''

    infile, tax_profile = infiles
    assert os.path.exists(tax_profile), 'No humann3 metagenome output found'
    
    outfile = P.snip(infile, '.fastq.gz', strip_path=True)
    outfile = os.path.join('humann3_mtx.dir',
                            outfile,
                            outfile + '_pathcoverage.tsv.gz')
    
    # make output directory
    outdir = os.path.abspath(os.path.dirname(outfile))
    if os.path.exists(outdir):
        shutil.rmtree(outdir)
    os.mkdir(outdir)

    tool = H.humann3(infile, outfile, **PARAMS)

    # Not sure if humann can take gzipped input
    tmpf = P.get_temp_filename('.')
    statement = "zcat %(tax_profile)s > %(tmpf)s" % locals()
    statement = ' && '.join([statement,
                             tool.buildStatement(),
                             tool.postProcess()])
                    
    P.run(statement,
          job_memory = PARAMS["humann3_job_memory"],
          job_threads = PARAMS["humann3_job_threads"],
          job_options = PARAMS.get('humann3_job_options',''))
    os.unlink(tmpf)


###############################################################################
# merge Humann3 output files
###############################################################################
@collate([runHumann3, runHumann3_metatranscriptome],
         regex("(humann3.dir|humann3_mtx.dir)/.+/.+_(pathcoverage|pathabundance|genefamilies).tsv.gz"),
         r"\1/merged_tables/merged_\2.tsv.gz")
def mergeHumannOutput(infiles, outfile):
    '''Merge respective output files from humann. Note this uses humann
    scripts which don't account for the metaphlan bugs list'''
    
    # Fetch the output filetype
    suffix = re.search(".+/.+_(pathcoverage|pathabundance|genefamilies).tsv.gz",
                       infiles[0]).group(1)
    assert suffix in ('pathcoverage', 'pathabundance', 'genefamilies')

    # Fetch the input directory
    outdir = os.path.dirname(outfile)
    indir = os.path.dirname(outdir)
    if not os.path.exists(outdir):
        os.mkdir(outdir)
    # need to remove merged dir  when re-running pipeline
    if os.path.exists(outfile):
        shutil.rmtree(outfile)

    outf = P.snip(outfile, '.gz')
    
    statement = ("humann_join_tables"
                 "  -i %(indir)s"
                 "  -s"
                 "  --file_name %(suffix)s"
                 "  -o %(outf)s &&"
                 " gzip %(outf)s")
    P.run(statement)
    
@transform(mergeHumannOutput,
           suffix('genefamilies.tsv.gz'),
           'KOs.tsv.gz')
def mapUniref2KOs(infile, outfile):
    '''Use humann regroup_table script to fetch KO for Uniref mapping'''

    statement = ("zcat %(infile)s |"
                 " humann_regroup_table"
                 "  --custom %(humann3_uniref_to_ko)s"
                 " 2> %(outfile)s.log |"
                 " gzip > %(outfile)s")
    P.run(statement,
          job_memory = PARAMS["humann3_postprocess_memory"],
          job_threads = PARAMS["humann3_postprocess_threads"])

@transform([mergeHumannOutput, mapUniref2KOs],
           suffix('.tsv.gz'),
           '_cpm.tsv.gz')
def renormalizeHumannOutput(infile, outfile):
    '''Renormalize to CPM'''

    statement = ("zcat %(infile)s |"
                 " humann_renorm_table"
                 "  --units cpm"
                 " 2> %(outfile)s.log |"
                 " gzip > %(outfile)s")
    P.run(statement,
          job_memory = PARAMS["humann3_postprocess_memory"],
          job_threads = PARAMS["humann3_postprocess_threads"])

###############################################################################
# Handle metaphlan output
###############################################################################
@follows(mkdir('metaphlan_output.dir'))
@collate(runHumann3,
         regex("humann3.dir/.+/.+_metaphlan_bugs_list.tsv.gz"),
         r"metaphlan_output.dir/merged_metaphlan_bugs_list.tsv.gz")
def mergeMetaphlanOutput(infiles, outfile):
    '''Merge respective output files from humann's internal metaphlan run.
    Note this uses metaphlan custom scripts.'''

    # Script can't handle compressed input
    infiles = [x for x in infiles]
    tmpdir = P.get_temp_dir('.')
    tmpfiles = [os.path.join(tmpdir, P.snip(x, '.gz', strip_path=True)) \
                for x in infiles]
    outf = P.snip(outfile, '.gz')

    statements = []
    for i in zip(infiles,tmpfiles):
        statements.append("zcat %s > %s" % i)

    statements = " && ".join(statements)
    in_tmp = " ".join(tmpfiles)
    statement = statements + " && " +\
        "merge_metaphlan_tables.py -o %(outf)s %(in_tmp)s" + " && " \
        "gzip %(outf)s"

    P.run(statement)

    shutil.rmtree(tmpdir)

@split(mergeMetaphlanOutput, "metaphlan_output.dir/metaphlan_*.tsv.gz")
def splitMetaphlanByTaxonomy(infile, outfiles):
    '''split merged metaphlan file by taxonomic levels'''

    # ruffus bug? split defaults to list as input
    assert len(infile) == 1
    infile = infile[0]
    
    statement = '''ocms_shotgun split_metaphlan -i %(infile)s -o metaphlan_output.dir'''
    P.run(statement)


###############################################################################
@follows("mergeHumannOutput",
         mkdir("report.dir"))
def build_report():
    '''
    render the rmarkdown report file
    '''
    reportdir = os.path.dirname(os.path.abspath(__file__))
    reportdir = os.path.join(reportdir, "pipeline_docs", "Rmd", "pipeline_humann3")
    reportfile = os.path.join(reportdir, "pipeline_humann3_report.Rmd")

    # decompress merged humann outputs
    # copy report template to report.dir and render report
    # recompress humann outputs
    statement = '''gunzip humann3.dir/merged_tables/merged*.gz metaphlan_output.dir/metaphlan*.gz;
                   cp %(reportfile)s report.dir;
                   R -e "rmarkdown::render('report.dir/pipeline_humann3_report.Rmd', output_file='pipeline_humann3_report.html', output_dir='report.dir')";
                   gzip humann3.dir/merged_tables/merged*.tsv metaphlan_output.dir/metaphlan*.tsv
                '''
    P.run(statement)


###############################################################################
# Generic pipeline tasks
@follows(runHumann3, runHumann3_metatranscriptome)
def runHumann():
    pass

@follows(renormalizeHumannOutput, splitMetaphlanByTaxonomy)
def full():
    pass


def main(argv=None):
    if argv is None:
        argv = sys.argv
    P.main(argv)


if __name__ == "__main__":
    sys.exit(P.main(sys.argv))    
