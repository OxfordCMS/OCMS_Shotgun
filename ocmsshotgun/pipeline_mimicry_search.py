"""
===========================
pipeline_mimicry_search.py
===========================

:Author: Holly Roach, Marcin Pekalski and Dominik Trzupek
:Tags: Python

Overview
========
This pipeline takes contig.fasta files as input and uses prokka to predict
open reading frames and generate protein assemblies. The .faa file outputs from
prokka are used to create a blast database, which is used to search for sequence
homology for a specified protein sequence. The full nucleotide contigs containing
the proteins identified with sequence homology are then extracted using the .tbl
and .ffn file outputs from prokka. These full nucleotide contigs are used as 
input for a nucleotide blast search to identify the most likely taxa that these
contigs correspond too. 

Prokka documentation: https://github.com/tseemann/prokka
BLAST+ documentation: https://www.ncbi.nlm.nih.gov/books/NBK279690/

Usage
=====
Script takes in all contigs.fasta files located in the input.dir, and runs
prokka on these files to return a protein assembly. The .faa files are used as
input for blast to create a blast database, which can then be searched for
sequence homology. Using the homology search output, the corresponding full
nucleotide contig fasta files are extracted and used as input for a blastn
search to identify the corresponding microbial taxa.


Example::
    ocms_shotgun pipeline_mimicry_search make full


Configuration
-------------
ocms_shotgun pipeline_mimicry_search config

Input files
-----------
Input files should be contigs.fasta files located in input.dir

Requirements
------------
prokka/1.14.5-gompi-2022b
BLAST+/2.14.0-gompi-2022b

Pipeline output
===============
01_prokka_output.dir contains a merged.faa containing protein CDS sequences for
all samples, as well as a folder for each sample which contains the individual
outputs for each sample generated by prokka:
    sample_id.faa - Protein FASTA file of the translated CDS sequences.
    sample_id.ffn - Nucleotide FASTA file of all the prediction transcripts
                    (CDS, rRNA, tRNA, tmRNA, misc_RNA)
    sample_id.fsa - Nucleotide FASTA file of the input contig sequences, used
                    by "tbl2asn" to create the .sqn file. It is mostly the same
                    as the .fna file, but with extra Sequin tags in the sequence
                    description lines.
    sample_id.gff - This is the master annotation in GFF3 format, containing
                    both sequences and annotations. It can be viewed directly
                    in Artemis or IGV.
    sample_id.sqn - An ASN1 format "Sequin" file for submission to Genbank. It
                    needs to be edited to set the correct taxonomy, authors,
                    related publication etc.
    sample_id.tsv - Tab-separated file of all features: locus_tag,ftype,len_bp,
                    gene,EC_number,COG,product
    sample_id.err - Unacceptable annotations - the NCBI discrepancy report.
    sample_id.fna - Nucleotide FASTA file of the input contig sequences.
    sample_id.gbk - This is a standard Genbank file derived from the master
                    .gff. If the input to prokka was a multi-FASTA, then this
                    will be a multi-Genbank, with one record for each sequence.
    sample_id.log - Contains all the output that Prokka produced during its run.
                    This is a record of what settings you used, even if the
                    --quiet option was enabled.
    sample_id.tbl - Feature Table file, used by "tbl2asn" to create the .sqn file.
    sample_id.txt - Statistics relating to the annotated features found.

02_blast_database.dir contains a blast database (db.pdb) that is generated from the
merged.faa file. Within this database a unique identifier is assigned to every
protein sequence within the database, thereby allowing you to associate every
sequence to a taxonomic node (through the taxid of the sequence).

03_blast_search.dir contains a folder for each epitope used as an input for the
blast homology search. This folder contains the fasta file corresponding to the 
epitope of interest as well as a .tsv file containing the blast search results.

04_find_mimicry_contigs.dir contains a fasta file containing the nucleotide 
sequences of the full contigs that were identified to contain a mimic.


Glossary
========

..glossary::


Code
====

"""

import sys
import re
from pathlib import Path
from ruffus import regex, follows, collate, mkdir, originate, split, subdivide, transform, suffix, add_inputs
from cgatcore import pipeline as P
import pandas as pd
from datetime import datetime


# get all fasta contig files within directory to process
FASTAFILES = "input.dir/*"
FASTAFILES_REGEX = regex(r"input\.dir\/(.+)\.fasta")

PARAMS = P.get_parameters(
    ["pipeline.yml"],
    defaults = {
        "blast_epitope_seq": "amino_acid_seq",
        "blast_epitope_name": "name"
    }
)


###############################################################################
# Create protein assemblies using prokka
###############################################################################

@follows(mkdir("01_prokka_output.dir"))
@subdivide(
    FASTAFILES,  
    FASTAFILES_REGEX,
    [
        r"01_prokka_output.dir/\1/\1.faa",
        r"01_prokka_output.dir/\1/\1.ffn",
        r"01_prokka_output.dir/\1/\1.fsa",
        r"01_prokka_output.dir/\1/\1.gff",
        r"01_prokka_output.dir/\1/\1.sqn",
        r"01_prokka_output.dir/\1/\1.tsv",
        r"01_prokka_output.dir/\1/\1.err",
        r"01_prokka_output.dir/\1/\1.fna",
        r"01_prokka_output.dir/\1/\1.gbk",
        r"01_prokka_output.dir/\1/\1.log",
        r"01_prokka_output.dir/\1/\1.tbl",
        r"01_prokka_output.dir/\1/\1.txt",
    ]
)

def run_prokka(infile, outfiles):
    """Use prokka to predict ORFs and generate protein assemblies"""

    # capture sample id from infile
    sample_id = re.search(r"input\.dir\/(.+)\.fasta", infile).group(1)

    # define output dir
    outdir = f"01_prokka_output.dir/{sample_id}"

    # create statment for running prokka
    statement = (
        "prokka"
        " --cpus 0"
        f" --prefix {sample_id}"
        f" --locustag {sample_id}"
        f" --outdir {outdir}"
        f" --force {infile}"
    )

    # create script for slurm job submission
    P.run(
        statement,
        job_threads=PARAMS["prokka_job_threads"],
        job_memory=PARAMS["prokka_job_memory"],
    )


###############################################################################
# Merge all protein assemblies into one fasta file
###############################################################################

@follows(run_prokka)
@collate(
    "01_prokka_output.dir/*/*.faa",
    regex("01_prokka_output.dir/.+/.+.faa"),
    "01_prokka_output.dir/merged.faa",
)
def merge_faa(infiles, outfile):
    """Concatenate each faa file into one file"""

    # create command line statment to concatenate files
    statement = f"cat {' '.join(infiles)} > {outfile}"

    # submit statement as a job
    P.run(statement)


###############################################################################
# Create an indexed blast database
###############################################################################

@follows(merge_faa, mkdir("02_blast_database.dir"))
@split(
    "01_prokka_output.dir/merged.faa",
    [
        "02_blast_database.dir/database*pdb",
        "02_blast_database.dir/database*phr",
        "02_blast_database.dir/database*pin",
        "02_blast_database.dir/database*pjs",
        "02_blast_database.dir/database*pog",
        "02_blast_database.dir/database*pos",
        "02_blast_database.dir/database*pot",
        "02_blast_database.dir/database*psq",
        "02_blast_database.dir/database*ptf",
        "02_blast_database.dir/database*pto"
    ]
)

def create_blastdb(infile, outfiles):
    """Create an indexed blast database"""

    # define name of outfile
    outfile = "02_blast_database.dir/database"

    # create command line statment to create a blast database
    statement = (
        "makeblastdb"
        f" -in {infile}" 
        " -parse_seqids" 
        " -dbtype prot" 
        f" -out {outfile}"
    )

    # submit statement as a job
    P.run(statement,
          job_memory = PARAMS["blast_db_job_memory"],
          job_threads = PARAMS["blast_db_job_threads"])


###############################################################################
# Create an fasta file containing epitope for homology search
###############################################################################

# return protein seq for epitope stored in pipeline.yml
epitope_seq = PARAMS["blast_epitope_seq"]

# return name for epitope stored in pipeline.yml
epitope_name = PARAMS["blast_epitope_name"]

# create name of fasta file to store epitope sequence within
epitope_file = f"{epitope_name}_epitope.faa"

# create folder name for outputs of homology search for specified epitope
output_folder = f"{epitope_name}_outputs"


@follows(create_blastdb, mkdir(f"03_blast_search.dir/{output_folder}"))
@originate(f"03_blast_search.dir/{output_folder}/{epitope_file}")
def create_epi_fasta(outfile, extras=[epitope_seq, epitope_file]):
    """Create fasta file containing protein sequence of epitope to perform
    homology search against"""

    # create fasta file containing epitope sequence
    epitope_fasta = open(f"03_blast_search.dir/{output_folder}/{epitope_file}", "w")
    epitope_fasta.write(f"{epitope_seq}")
    epitope_fasta.close()


###############################################################################
# Search for sequence homology to defined epitope
###############################################################################
@follows(create_epi_fasta)
@transform(
    create_epi_fasta,
    suffix(".faa"),
    r"\1_blast_search.tsv"
)

def blast_search(infile, outfile):
    """Run a homology search for a specific epitope"""

    # define indexed database to use for blast search
    index = "02_blast_database.dir/database"

    # define parameters to use in the statement
    threads = PARAMS["blast_search_job_threads"]

    # create command line statment to run blast search
    statement = (
        "export BLASTDB=02_blast_database.dir &&" 
        " blastp" 
        f" -query {infile}" 
        f" -db {index}" 
        f" -out {outfile}" 
        f" -num_threads {threads}"
        " -ungapped" 
        " -comp_based_stats F" 
        " -matrix BLOSUM62" 
        " -evalue 10000" 
        " -num_alignments 100000" 
        " -outfmt '7 qseqid sseqid pident length mismatch gapopen qstart qend sstart send evalue bitscore btop sseq stitle'"
        " -html"
    )

    # submit statement as a job
    P.run(statement,
          job_memory = PARAMS["blast_search_job_memory"],
          job_threads = PARAMS["blast_search_job_threads"])
    

###############################################################################
# Find nucleotide contigs that contain proteins identified in blast search
###############################################################################]
@follows(blast_search, mkdir(f"04_find_mimicry_contigs.dir/{epitope_name}"))
@transform(
    "01_prokka_output.dir/*/*.fna",
    regex("01_prokka_output.dir/(.+)/.+.fna"),
    rf"04_find_mimicry_contigs.dir/{epitope_name}/\1_mimicry_contigs.fasta",
)

def extract_contigs(infile, outfile):
    """Find the contig nucelotide sequence containing the proteins identified
    by blast homology search"""

    # define file path to nucleotide FASTA file of the input contig sequences
    # generated by prokka
    # >Feature ids within tbl_file correspond to the contig ids within this file
    fna_file = Path(infile)

    # define file path to "Feature Table file" generated by prokka
    # this contains information relating to each annoated contig (i.e. which
    # nucleotides within the contig correspond to which annoted protein)
    tbl_file = Path(re.sub("fna", "tbl", str(infile)))

    # define file path to the blast search results
    mimicry_results = f"03_blast_search.dir/{output_folder}/{epitope_name}_epitope_blast_search.tsv"

    # define sampleid
    sample_id = re.search(r"01_prokka_output.dir/(.+)/.+.fna", str(infile)).group(1)

    print(f"# {datetime.now().strftime('%Y-%m-%d %H:%M:%S,%f')[:-3]} INFO main execution - running extract_contigs() for {sample_id}", flush=True)                                

    # read in the blast mimicry search results as a dataframe
    mimicry_df = pd.read_csv(
        mimicry_results,
        sep="\t",
        comment="#",
        names=[
            "query_id",
            "subject_id",
            "perc_identity",
            "alignment_length",
            "mismatches",
            "gap opens",
            "q_start",
            "q_end",
            "s_start",
            "s_end",
            "evalue",
            "bit_score",
            "BTOP",
            "subject_seq",
            "subject_title",
            "subject_tax_id",
        ],
    )

    # create dict containing the data stored in fna_file
    # key = contig id
    # value = contig nucleiotide fasta seq
    # {'contig_id': 'TTCTAATGAATGCTATCTATA....',
    # }
    fna_dict = {}
    for id_seq in fna_file.read_text().split(">")[1:]:
        id, *seq = id_seq.split("\n")
        fna_dict[id] = "".join(seq)

    # create a nested dict containg the information stored in the tbl_file
    # key = locus_tag (unique id referenced as subject_id in blast search output)
    # value = dict containing the corresponding protein name and contig id
    # {'locus_tag': {
    #     'protein_name': '',
    #     'contig_id': ``
    #     },
    # }
    tbl_dict = {}
    for contig in tbl_file.read_text().split(">Feature ")[1:]:
        contig_id = locus_tag = protein_name = None

        for i, line in enumerate(contig.splitlines()):
            # First line, starts with feature, contains the contig ID
            if i == 0:
                contig_id = line.strip()
                continue

            # Extract locus tag and product location
            if line.strip().startswith("locus_tag"):
                locus_tag = line.strip().split("\t")[1]
                continue

            if line.strip().startswith("product"):
                protein_name = line.strip().split("\t")[1]

                # Now have captured product
                # in this locus block, so save info then reset
                tbl_dict[locus_tag] = {"protein_name": protein_name, "contig_id": contig_id}
                # Reset for next locus block
                locus_tag = protein_name = None

    # convert dict into data frame
    tbl_df = pd.DataFrame([{"locus_tag": k, **v} for k, v in tbl_dict.items()])

    del tbl_dict

    # convert dict into data frame
    fna_df = pd.DataFrame([{"contig_id": k, "contig_seq": v} for k, v in fna_dict.items()])

    del fna_dict

    # create merged data frame for proteins identified in mimcry search
    # subject_id | protein_name | contig_id | contig_seq
    df = (
        mimicry_df
        .loc[:, ["subject_id"]]
        .merge(tbl_df, left_on="subject_id", right_on="locus_tag", how="inner")
        .drop(columns=["locus_tag"])
        .merge(fna_df, on="contig_id", how="left")
    )

    # create nucleotide fasta file containing contigs identified in blast search
    # >subject_id contig_id
    # contig_seq
    blast_hits_fasta = []

    for index, row in df.iterrows():
        # extract info for relating to each blast hit
        subject_id = row['subject_id']
        contig_id = row['contig_id']
        contig_seq = row['contig_seq']

        # segment the contig_seq into multiple lists containing 80 characters
        contig_seq = "\n".join(contig_seq[0+i:80+i] for i in range(0, len(contig_seq), 80))

        # create fasta file for each blast hit
        fasta = (f">{subject_id}__{contig_id}\n{contig_seq}")
        
        # add fasta to a list containing all contigs fastas
        blast_hits_fasta.append(fasta)
    
    # save output
    with open(outfile, 'w') as output:
        # Join the list elements into a single string with a newline character
        blast_hits_fasta = '\n'.join(blast_hits_fasta)
        # Write the data to the file
        output.write(blast_hits_fasta)

    print(f"                                             extarcted {len(df)} contigs encoding proteins with potenital mimicry", flush=True)

###############################################################################
@follows(extract_contigs)
def full():
    pass

def main(argv=None):
    if argv is None:
        argv = sys.argv
    P.main(argv)


if __name__ == "__main__":
    sys.exit(P.main(sys.argv))  