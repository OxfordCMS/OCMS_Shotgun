"""
===========================
pipeline_mimicry_search.py
===========================

:Author: Holly Roach, Marcin Pekalski and Dominik Trzupek
:Tags: Python

Overview
========
This pipeline takes contig.fasta files as input and uses prokka to predict
open reading frames and generate protein assemblies. The .faa file outputs from
prokka are used to create a blast database, which is used to search for sequence
homology for a specified protein sequence.

Prokka documentation: https://github.com/tseemann/prokka
BLAST+ documentation: https://www.ncbi.nlm.nih.gov/books/NBK279690/

Usage
=====
Script takes in all contigs.fasta files located in the input.dir, and runs
prokka on these files to return a protein assembly. The .faa files are used as
input for blast to create a blast database, which can then be searched for
 sequence homology.


Example::
    ocms_shotgun pipeline_mimicry_search make full


Configuration
-------------
ocms_shotgun pipeline_mimicry_search config

Input files
-----------
Input files should be contigs.fasta files located in input.dir

Requirements
------------
prokka/1.14.5-gompi-2022b
BLAST+/2.14.0-gompi-2022b

Pipeline output
===============
01_prokka_output.dir contains a merged.faa containing protein CDS sequences for
all samples, as well as a folder for each sample which contains the individual
outputs for each sample generated by prokka:
    sample_id.faa - Protein FASTA file of the translated CDS sequences.
    sample_id.ffn - Nucleotide FASTA file of all the prediction transcripts
                    (CDS, rRNA, tRNA, tmRNA, misc_RNA)
    sample_id.fsa - Nucleotide FASTA file of the input contig sequences, used
                    by "tbl2asn" to create the .sqn file. It is mostly the same
                    as the .fna file, but with extra Sequin tags in the sequence
                    description lines.
    sample_id.gff - This is the master annotation in GFF3 format, containing
                    both sequences and annotations. It can be viewed directly
                    in Artemis or IGV.
    sample_id.sqn - An ASN1 format "Sequin" file for submission to Genbank. It
                    needs to be edited to set the correct taxonomy, authors,
                    related publication etc.
    sample_id.tsv - Tab-separated file of all features: locus_tag,ftype,len_bp,
                    gene,EC_number,COG,product
    sample_id.err - Unacceptable annotations - the NCBI discrepancy report.
    sample_id.fna - Nucleotide FASTA file of the input contig sequences.
    sample_id.gbk - This is a standard Genbank file derived from the master
                    .gff. If the input to prokka was a multi-FASTA, then this
                    will be a multi-Genbank, with one record for each sequence.
    sample_id.log - Contains all the output that Prokka produced during its run.
                    This is a record of what settings you used, even if the
                    --quiet option was enabled.
    sample_id.tbl - Feature Table file, used by "tbl2asn" to create the .sqn file.
    sample_id.txt - Statistics relating to the annotated features found.

02_blast_database.dir contains a blast database (db.pdb) that is generated from the
merged.faa file. Within this database a unique identifier is assigned to every
protein sequencing within the database, thereby allowing you to associate every
sequence to a taxonomic node (through the taxid of the sequence).

homolgy_search.dir contains


Glossary
========

..glossary::


Code
====

"""

import sys
import re
from pathlib import Path
from ruffus import regex, follows, collate, mkdir, originate, split, subdivide
from cgatcore import pipeline as P

# get all fasta contig files within directory to process
FASTAFILES = "input.dir/*contigs.fasta"
FASTAFILES_REGEX = regex(r"input\.dir\/(\S+?)_.+\.fasta")

PARAMS = P.get_parameters(["pipeline.yml"])


###############################################################################
# Create protein assemblies using prokka
###############################################################################

@follows(mkdir("01_prokka_output.dir"))
@subdivide(
    FASTAFILES,  
    FASTAFILES_REGEX,
    [
        r"01_prokka_output.dir/\1/\1.faa",
        r"01_prokka_output.dir/\1/\1.ffn",
        r"01_prokka_output.dir/\1/\1.fsa",
        r"01_prokka_output.dir/\1/\1.gff",
        r"01_prokka_output.dir/\1/\1.sqn",
        r"01_prokka_output.dir/\1/\1.tsv",
        r"01_prokka_output.dir/\1/\1.err",
        r"01_prokka_output.dir/\1/\1.fna",
        r"01_prokka_output.dir/\1/\1.gbk",
        r"01_prokka_output.dir/\1/\1.log",
        r"01_prokka_output.dir/\1/\1.tbl",
        r"01_prokka_output.dir/\1/\1.txt",
    ]
)

def run_prokka(infile, outfiles):
    """Use prokka to predict ORFs and generate protein assemblies"""

    # capture sample id from infile
    sample_id = re.search(r"input\.dir\/(\S+?)_.+\.fasta", infile).group(1)

    # define output dir
    outdir = f"01_prokka_output.dir/{sample_id}"

    # create statment for running prokka
    statement = (
        "prokka"
        " --cpus 0"
        f" --prefix {sample_id}"
        f" --locustag {sample_id}"
        f" --outdir {outdir}"
        f" --force {infile}"
    )

    # create script for slurm job submission
    P.run(
        statement,
        job_threads=PARAMS["prokka_job_threads"],
        job_memory=PARAMS["prokka_job_memory"],
    )


###############################################################################
# Merge all protein assemblies into one fasta file
###############################################################################

@follows(run_prokka)
@collate(
    "01_prokka_output.dir/*/*.faa",
    regex("01_prokka_output.dir/.+/.+.faa"),
    "01_prokka_output.dir/merged.faa",
)
def merge_faa(infiles, outfile):
    """Concatenate each faa file into one file"""

    # create command line statment to concatenate files
    statement = f"cat {' '.join(infiles)} > {outfile}"

    # submit statement as a job
    P.run(statement)


###############################################################################
# Create an indexed blast database
###############################################################################

@follows(merge_faa, mkdir("02_blast_database.dir"))
@split(
    "01_prokka_output.dir/merged.faa",
    [
        "02_blast_database.dir/database.pdb",
        "02_blast_database.dir/database.phr",
        "02_blast_database.dir/database.pin",
        "02_blast_database.dir/database.pjs",
        "02_blast_database.dir/database.pog",
        "02_blast_database.dir/database.pos",
        "02_blast_database.dir/database.pot",
        "02_blast_database.dir/database.psq",
        "02_blast_database.dir/database.ptf",
        "02_blast_database.dir/database.pto"
    ]
)

def create_blastdb(infile, outfiles):
    """Create an indexed blast database"""

    # define name of outfile
    outfile = "02_blast_database.dir/database"

    # create command line statment to create a blast database
    statement = (
        "makeblastdb"
        f" -in {infile}" 
        " -parse_seqids" 
        " -dbtype prot" 
        f" -out {outfile}"
    )

    # submit statement as a job
    P.run(statement,
          job_memory = PARAMS["blast_db_job_memory"],
          job_threads = PARAMS["blast_db_job_threads"])


###############################################################################
# Create an fasta file containing epitope for homology search
###############################################################################

# return protein seq for epitope stored in pipeline.yml
epitope_seq = PARAMS["blast_epitope_seq"]

# return name for epitope stored in pipeline.yml
epitope_name = PARAMS["blast_epitope_name"]

# create name of fasta file to store epitope sequence within
epitope_file = f"{epitope_name}_epitope.faa"

# create folder name for outputs of homology search for specified epitope
output_folder = f"{epitope_name}_outputs"


@follows(create_blastdb, mkdir(f"03_blast_search.dir/{output_folder}"))
@originate(f"03_blast_search.dir/{output_folder}/{epitope_file}")
def create_epi_fasta(outfile, extras=[epitope_seq, epitope_file]):
    """Create fasta file containing protein sequence of epitope to perform
    homology search against"""

    # create fasta file containing epitope sequence
    epitope_fasta = open(f"03_blast_search.dir/{output_folder}/{epitope_file}", "w")
    epitope_fasta.write(f"{epitope_seq}")
    epitope_fasta.close()


###############################################################################
# Search for sequence homology to defined epitope
###############################################################################

@follows(create_epi_fasta)
@originate(f"03_blast_search.dir/{output_folder}/blast_search.tsv")
def blast_search(outfile, extras=[output_folder, epitope_file]):
    """Run a homology search for a specific epitope"""

    # define parameters to use in the statement
    threads = PARAMS["blast_search_job_threads"]

    # create command line statment to run blast search
    statement = (
        "export BLASTDB=02_blast_database.dir &&" 
        " blastp" 
        f" -query 03_blast_search.dir/{output_folder}/{epitope_file}" 
        " -db database" 
        f" -out {outfile}" 
        f" -num_threads {threads}"
        " -ungapped" 
        " -comp_based_stats F" 
        " -matrix BLOSUM62" 
        " -evalue 10000" 
        " -num_alignments 100000" 
        " -outfmt '7 qseqid sseqid pident length mismatch gapopen qstart qend sstart send evalue bitscore btop stitle'"
    )

    # submit statement as a job
    P.run(statement,
          job_memory = PARAMS["blast_search_job_memory"],
          job_threads = PARAMS["blast_search_job_threads"])


@follows(blast_search)
def full():
    pass

def main(argv=None):
    if argv is None:
        argv = sys.argv
    P.main(argv)


if __name__ == "__main__":
    sys.exit(P.main(sys.argv))  