"""
===========================
pipeline_mimicry_search.py
===========================

:Author: Holly Roach, Marcin Pekalski and Dominik Trzupek
:Tags: Python

Overview
========
This pipeline takes contig.fasta files as input and uses prokka to predict
open reading frames and generate protein assemblies. The .faa file outputs from
prokka are used to create a blast database, which is used to search for sequence
homology for a specified protein sequence. The full nucleotide contigs containing
the proteins identified with sequence homology are then extracted using the .tbl
and .ffn file outputs from prokka. These full nucleotide contigs are used as 
input for a nucleotide blast search to identify the most likely taxa that these
contigs correspond too. 

Prokka documentation: https://github.com/tseemann/prokka
BLAST+ documentation: https://www.ncbi.nlm.nih.gov/books/NBK279690/

Usage
=====
Script takes in all contigs.fasta files located in the input.dir, and runs
prokka on these files to return a protein assembly as well as other outputs. 
The .faa files are used as input for blast to create a blast database, which 
can then be searched for sequence homology. Using the homology search output, 
the corresponding full nucleotide contig fasta files are extracted and used 
as input for a blastn search to identify the corresponding microbial taxa.


Example::
    ocms_shotgun pipeline_mimicry_search make full


Configuration
-------------
ocms_shotgun pipeline_mimicry_search config

Input files
-----------
Input files should be contigs.fasta files located in input.dir

Requirements
------------
prokka/1.14.5-gompi-2022b
BLAST+/2.14.0-gompi-2022b

Pipeline output
===============
01_prokka_output.dir contains a merged.faa containing protein CDS sequences for
all samples, as well as a folder for each sample which contains the individual
outputs for each sample generated by prokka:
    sample_id.faa - Protein FASTA file of the translated CDS sequences.
    sample_id.ffn - Nucleotide FASTA file of all the prediction transcripts
                    (CDS, rRNA, tRNA, tmRNA, misc_RNA)
    sample_id.fsa - Nucleotide FASTA file of the input contig sequences, used
                    by "tbl2asn" to create the .sqn file. It is mostly the same
                    as the .fna file, but with extra Sequin tags in the sequence
                    description lines.
    sample_id.gff - This is the master annotation in GFF3 format, containing
                    both sequences and annotations. It can be viewed directly
                    in Artemis or IGV.
    sample_id.sqn - An ASN1 format "Sequin" file for submission to Genbank. It
                    needs to be edited to set the correct taxonomy, authors,
                    related publication etc.
    sample_id.tsv - Tab-separated file of all features: locus_tag,ftype,len_bp,
                    gene,EC_number,COG,product
    sample_id.err - Unacceptable annotations - the NCBI discrepancy report.
    sample_id.fna - Nucleotide FASTA file of the input contig sequences.
    sample_id.gbk - This is a standard Genbank file derived from the master
                    .gff. If the input to prokka was a multi-FASTA, then this
                    will be a multi-Genbank, with one record for each sequence.
    sample_id.log - Contains all the output that Prokka produced during its run.
                    This is a record of what settings you used, even if the
                    --quiet option was enabled.
    sample_id.tbl - Feature Table file, used by "tbl2asn" to create the .sqn file.
    sample_id.txt - Statistics relating to the annotated features found.

02_blast_database.dir contains a blast database (db.pdb) that is generated from the
merged.faa file. Within this database a unique identifier is assigned to every
protein sequence within the database, thereby allowing you to associate every
sequence to a taxonomic node (through the taxid of the sequence).

03_blast_search.dir contains a folder for each epitope used as an input for the
blast homology search. This folder contains the fasta file corresponding to the 
epitope of interest as well as a .tsv file containing the blast search results.

04_find_mimicry_contigs.dir contains a folder for each mimicry epitope, which 
contains a fasta file for each sample containing the nucleotide sequences of
the full contigs that were identified to contain a protein mimic.

05_contig_blastn.dir contains the outputs of the blastn sequence homolgy
search of all contigs containing.

Glossary
========

..glossary::


Code
====

"""

import sys
import re
from pathlib import Path
from ruffus import regex, follows, collate, mkdir, originate, split, subdivide, transform, suffix, add_inputs
from cgatcore import pipeline as P
import pandas as pd
import numpy as np
from datetime import datetime


# get all fasta contig files within directory to process
FASTAFILES = "input.dir/*"
FASTAFILES_REGEX = regex(r"input\.dir\/(.+)\.fasta")

PARAMS = P.get_parameters(
    ["pipeline.yml"],
    defaults = {
        "blast_epitope_seq": "amino_acid_seq",
        "blast_epitope_name": "name"
    }
)


###############################################################################
# Create protein assemblies using prokka
###############################################################################

@follows(mkdir("01_prokka_output.dir"))
@subdivide(
    FASTAFILES,  
    FASTAFILES_REGEX,
    [
        r"01_prokka_output.dir/\1/\1.faa",
        r"01_prokka_output.dir/\1/\1.ffn",
        r"01_prokka_output.dir/\1/\1.fsa",
        r"01_prokka_output.dir/\1/\1.gff",
        r"01_prokka_output.dir/\1/\1.sqn",
        r"01_prokka_output.dir/\1/\1.tsv",
        r"01_prokka_output.dir/\1/\1.err",
        r"01_prokka_output.dir/\1/\1.fna",
        r"01_prokka_output.dir/\1/\1.gbk",
        r"01_prokka_output.dir/\1/\1.log",
        r"01_prokka_output.dir/\1/\1.tbl",
        r"01_prokka_output.dir/\1/\1.txt",
    ]
)

def run_prokka(infile, outfiles):
    """Use prokka to predict ORFs and generate protein assemblies"""

    # capture sample id from infile
    sample_id = re.search(r"input\.dir\/(.+)\.fasta", infile).group(1)

    # define output dir
    outdir = f"01_prokka_output.dir/{sample_id}"

    # create statment for running prokka
    statement = (
        "prokka"
        " --cpus 0"
        f" --prefix {sample_id}"
        f" --locustag {sample_id}"
        f" --outdir {outdir}"
        f" --force {infile}"
    )

    # create script for slurm job submission
    P.run(
        statement,
        job_threads=PARAMS["prokka_job_threads"],
        job_memory=PARAMS["prokka_job_memory"],
    )


###############################################################################
# Merge all protein assemblies into one fasta file
###############################################################################

@follows(run_prokka)
@collate(
    "01_prokka_output.dir/*/*.faa",
    regex("01_prokka_output.dir/.+/.+.faa"),
    "01_prokka_output.dir/merged.faa",
)
def merge_faa(infiles, outfile):
    """Concatenate each faa file into one file"""

    # create command line statment to concatenate files
    statement = f"cat {' '.join(infiles)} > {outfile}"

    # submit statement as a job
    P.run(statement)


###############################################################################
# Create an indexed blast database
###############################################################################

@follows(merge_faa, mkdir("02_blast_database.dir"))
@split(
    "01_prokka_output.dir/merged.faa",
    [
        "02_blast_database.dir/database*pdb",
        "02_blast_database.dir/database*phr",
        "02_blast_database.dir/database*pin",
        "02_blast_database.dir/database*pjs",
        "02_blast_database.dir/database*pog",
        "02_blast_database.dir/database*pos",
        "02_blast_database.dir/database*pot",
        "02_blast_database.dir/database*psq",
        "02_blast_database.dir/database*ptf",
        "02_blast_database.dir/database*pto"
    ]
)

def create_blastdb(infile, outfiles):
    """Create an indexed blast database"""

    # define name of outfile
    outfile = "02_blast_database.dir/database"

    # create command line statment to create a blast database
    statement = (
        "makeblastdb"
        f" -in {infile}" 
        " -parse_seqids" 
        " -dbtype prot" 
        f" -out {outfile}"
    )

    # submit statement as a job
    P.run(statement,
          job_memory = PARAMS["blast_db_job_memory"],
          job_threads = PARAMS["blast_db_job_threads"])


###############################################################################
# Create an fasta file containing epitope for homology search
###############################################################################

# return protein seq for epitope stored in pipeline.yml
epitope_seq = PARAMS["blast_epitope_seq"]

# return name for epitope stored in pipeline.yml
epitope_name = PARAMS["blast_epitope_name"]

# create name of fasta file to store epitope sequence within
epitope_file = f"{epitope_name}_epitope.faa"

# create folder name for outputs of homology search for specified epitope
output_folder = f"{epitope_name}_outputs"


@follows(create_blastdb, mkdir(f"03_blast_search.dir/{output_folder}"))
@originate(f"03_blast_search.dir/{output_folder}/{epitope_file}")
def create_epi_fasta(outfile, extras=[epitope_seq, epitope_file]):
    """Create fasta file containing protein sequence of epitope to perform
    homology search against"""

    # create fasta file containing epitope sequence
    epitope_fasta = open(f"03_blast_search.dir/{output_folder}/{epitope_file}", "w")
    epitope_fasta.write(f"{epitope_seq}")
    epitope_fasta.close()


###############################################################################
# Search for sequence homology to defined epitope
###############################################################################
@follows(create_epi_fasta)
@transform(
    create_epi_fasta,
    suffix(".faa"),
    r"\1_blast_search.tsv"
)

def blast_search(infile, outfile):
    """Run a homology search for a specific epitope"""

    # define indexed database to use for blast search
    index = "02_blast_database.dir/database"

    # define parameters to use in the statement
    threads = PARAMS["blast_search_job_threads"]

    # create command line statment to run blast search
    statement = (
        "export BLASTDB=02_blast_database.dir &&" 
        " blastp" 
        f" -query {infile}" 
        f" -db {index}" 
        f" -out {outfile}" 
        f" -num_threads {threads}"
        " -ungapped" 
        " -comp_based_stats F" 
        " -matrix BLOSUM62" 
        " -evalue 10000" 
        " -num_alignments 100000" 
        " -outfmt '7 qseqid sseqid pident length mismatch gapopen qstart qend sstart send evalue bitscore btop sseq stitle'"
        " -html"
    )

    # submit statement as a job
    P.run(statement,
          job_memory = PARAMS["blast_search_job_memory"],
          job_threads = PARAMS["blast_search_job_threads"])
    

###############################################################################
# Find nucleotide contigs that contain proteins identified in blast search
###############################################################################]
@follows(blast_search, mkdir(f"04_find_mimicry_contigs.dir/{output_folder}"))
@transform(
    "01_prokka_output.dir/*/*.fna",
    regex("01_prokka_output.dir/(.+)/.+.fna"),
    rf"04_find_mimicry_contigs.dir/{output_folder}/\1_mimicry_contigs.fasta",
)

def extract_contigs(infile, outfile):
    """Find the contig nucelotide sequence containing the proteins identified
    by blast homology search"""

    # define file path to nucleotide FASTA file of the input contig sequences
    # generated by prokka
    # >Feature ids within tbl_file correspond to the contig ids within this file
    fna_file = Path(infile)

    # define file path to "Feature Table file" generated by prokka
    # this contains information relating to each annoated contig (i.e. which
    # nucleotides within the contig correspond to which annoted protein)
    tbl_file = Path(re.sub("fna", "tbl", str(infile)))

    # define file path to the blast search results
    mimicry_results = f"03_blast_search.dir/{output_folder}/{epitope_name}_epitope_blast_search.tsv"

    # define sampleid
    sample_id = re.search(r"01_prokka_output.dir/(.+)/.+.fna", str(infile)).group(1)

    print(f"# {datetime.now().strftime('%Y-%m-%d %H:%M:%S,%f')[:-3]} INFO main execution - running extract_contigs() for {sample_id}", flush=True)                                

    # read in the blast mimicry search results as a dataframe
    mimicry_df = pd.read_csv(
        mimicry_results,
        sep="\t",
        comment="#",
        names=[
            "query_id",
            "subject_id",
            "perc_identity",
            "alignment_length",
            "mismatches",
            "gap opens",
            "q_start",
            "q_end",
            "s_start",
            "s_end",
            "evalue",
            "bit_score",
            "BTOP",
            "subject_seq",
            "subject_title",
            "subject_tax_id",
        ],
    )

    # create dict containing the data stored in fna_file
    # key = contig id
    # value = contig nucleiotide fasta seq
    # {'contig_id': 'TTCTAATGAATGCTATCTATA....',
    # }
    fna_dict = {}
    for id_seq in fna_file.read_text().split(">")[1:]:
        id, *seq = id_seq.split("\n")
        fna_dict[id] = "".join(seq)

    # create a nested dict containg the information stored in the tbl_file
    # key = locus_tag (unique id referenced as subject_id in blast search output)
    # value = dict containing the corresponding protein name and contig id
    # {'locus_tag': {
    #     'protein_name': '',
    #     'contig_id': ``
    #     },
    # }
    tbl_dict = {}
    for contig in tbl_file.read_text().split(">Feature ")[1:]:
        contig_id = locus_tag = protein_name = None

        for i, line in enumerate(contig.splitlines()):
            # First line, starts with feature, contains the contig ID
            if i == 0:
                contig_id = line.strip()
                continue

            # Extract locus tag and product location
            if line.strip().startswith("locus_tag"):
                locus_tag = line.strip().split("\t")[1]
                continue

            if line.strip().startswith("product"):
                protein_name = line.strip().split("\t")[1]

                # Now have captured product
                # in this locus block, so save info then reset
                tbl_dict[locus_tag] = {"protein_name": protein_name, "contig_id": contig_id}
                # Reset for next locus block
                locus_tag = protein_name = None

    # convert dict into data frame
    tbl_df = pd.DataFrame([{"locus_tag": k, **v} for k, v in tbl_dict.items()])

    del tbl_dict

    # convert dict into data frame
    fna_df = pd.DataFrame([{"contig_id": k, "contig_seq": v} for k, v in fna_dict.items()])

    del fna_dict

    # create merged data frame for proteins identified in mimcry search
    # subject_id | protein_name | contig_id | contig_seq
    df = (
        mimicry_df
        .loc[:, ["subject_id"]]
        .merge(tbl_df, left_on="subject_id", right_on="locus_tag", how="inner")
        .drop(columns=["locus_tag"])
        .merge(fna_df, on="contig_id", how="left")
    )

    # create nucleotide fasta file containing contigs identified in blast search
    # >subject_id contig_id
    # contig_seq
    blast_hits_fasta = []

    for index, row in df.iterrows():
        # extract info for relating to each blast hit
        subject_id = row['subject_id']
        contig_id = row['contig_id']
        contig_seq = row['contig_seq']

        # segment the contig_seq into multiple lists containing 80 characters
        contig_seq = "\n".join(contig_seq[0+i:80+i] for i in range(0, len(contig_seq), 80))

        # create fasta file for each blast hit
        fasta = (f">{subject_id}__{contig_id}\n{contig_seq}")
        
        # add fasta to a list containing all contigs fastas
        blast_hits_fasta.append(fasta)
    
    # save output
    with open(outfile, 'w') as output:
        # Join the list elements into a single string with a newline character
        blast_hits_fasta = '\n'.join(blast_hits_fasta)
        # Write the data to the file
        output.write(blast_hits_fasta)

    print(f"                                             extarcted {len(df)} contigs encoding proteins with potenital mimicry toward {epitope_name}", flush=True)


###############################################################################
# Blastn sequence homology homology to identify taxa
###############################################################################
@follows(extract_contigs, mkdir(f"05_contig_blastn.dir/{output_folder}"))

@transform(
    f"04_find_mimicry_contigs.dir/{output_folder}/*_mimicry_contigs.fasta",
    regex(f"04_find_mimicry_contigs.dir/{output_folder}/(.+)_mimicry_contigs.fasta"),
    rf"05_contig_blastn.dir/{output_folder}/\1_contig_blastn.tsv",
)

def contig_taxa_blastn(infile, outfile):
    """Run nucleotide homology search for entire contig to idenifty taxonmy"""

    # define parameters to use in the statement
    threads = PARAMS["blast_search_job_threads"]

    # define path to downloaded blast nucleotide database
    nt_database = PARAMS["blast_nt_database"]

    # create command line statment to run blast search 
    statement = (
        "blastn"
        f" -query {infile}"
        f" -db {nt_database}"
        f" -out {outfile}" 
        f" -num_threads {threads}"
        " -num_alignments 50"
        " -num_descriptions 50"
        " -outfmt '7 qaccver saccver length evalue bitscore pident qcovs stitle'"
    )


    # submit statement as a job
    P.run(statement,
          job_memory = PARAMS["blast_search_job_memory"],
          job_threads = PARAMS["blast_search_job_threads"])
    
###############################################################################
# Extract top blast hit for each mimicry hit
###############################################################################
@follows(contig_taxa_blastn, mkdir(f"06_top_taxa_blastn.dir/{output_folder}"))

@transform(
    f"05_contig_blastn.dir/{output_folder}/*_contig_blastn.tsv",
    regex(f"05_contig_blastn.dir/{output_folder}/(.+)_contig_blastn.tsv"),
    rf"06_top_taxa_blastn.dir/{output_folder}/\1_top_taxa_blastn.tsv",
)

def contig_top_taxa(infile, outfile):
    """Find the best blast hit for each contig based on bitscore, evalue, 
    alignment length, and percentage idenity"""

    # define sampleid
    sample_id = re.search(rf"05_contig_blastn.dir/{output_folder}/(.+)_contig_blastn.tsv", str(infile)).group(1)

    print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S,%f')[:-3]} INFO main execution - finding top blast hits of each mimicry contig for sample {sample_id}", flush=True) 

    # define file path to contig blast results
    # this contains the results of the contig blast search
    contig_blastn = Path(infile)

    # use genarator to extract list of query ids and store in dataframe
    queries_df = []
    for line in contig_blastn.open("r"):
        if line.startswith("# Query: "):
            queries_df.append(line.removeprefix("# Query: ").strip())

    queries_df = pd.DataFrame({"query": queries_df})

    # open contig blastn results as dataframe
    contig_blastn_df = pd.read_csv(
        contig_blastn,
        sep="\t",
        comment="#",
        names=[
            "query",
            "subject_acc.ver",
            "alignment_length",
            "evalue",
            "bit_score",
            "perc_identity",
            "perc_query_coverage_per_subject",
            "subject_title",
        ],
        dtype={
            "query": str,
            "subject_acc.ver": str,
            "alignment_length": np.float64,
            "evalue": np.float64,
            "bit_score": np.float64,
            "perc_identity": np.float64,
            "perc_query_coverage_per_subject": np.float64,
            "subject_title": str,
        },
    )

    # find number of hits per query
    num_hits = contig_blastn_df.groupby("query", as_index=False).agg(
        total_hits=("query", "count")
    )

    # find the best hit per query
    # based on bit-score (highest), eval (lowest), length (longest)
    top_hits = (
        contig_blastn_df.sort_values(
            by=["bit_score", "evalue", "alignment_length", "perc_identity"],
            ascending=[False, True, False, False],
            na_position="last",
        )
        .groupby("query", as_index=False)
        .head(1)
    )

    # find number of hits for best scoring taxa per query
    taxa_hits = (
        top_hits[["query", "subject_acc.ver"]].merge(
            contig_blastn_df[["query", "subject_acc.ver"]],
            on=["query", "subject_acc.ver"],
            how="left",
        )
        .groupby(["query", "subject_acc.ver"], as_index=False)
        .agg(
            top_taxa_hits = ("subject_acc.ver", "count")
        )
    )

    # add number of hits per query
    final_df = pd.merge(
        queries_df,
        num_hits,
        on="query",
        how="left"
    )

    # add number of hits for each top blast hit per query
    final_df = pd.merge(
        final_df,
        taxa_hits,
        on="query",
        how="left"
    )

    # add data relating to the top blast hit per query
    final_df = pd.merge(
        final_df,
        top_hits,
        on=["query", "subject_acc.ver"],
        how="left"
    )

    # save output
    final_df.to_csv(outfile, sep="\t", na_rep='NULL')

    print(f"                                             found top blast hits for {len(final_df)} contigs encoding proteins with potenital mimicry toward {epitope_name}", flush=True)

                    
###############################################################################
# Merge all protein assemblies into one fasta file
###############################################################################
@follows(contig_top_taxa)
@collate(
    f"06_top_taxa_blastn.dir/{output_folder}/*_top_taxa_blastn.tsv",
    regex(f"06_top_taxa_blastn.dir/{output_folder}/(.+)_top_taxa_blastn.tsv"),
    f"06_top_taxa_blastn.dir/{output_folder}/merged_top_taxa_blastn.tsv",
)

def merge_top_taxa(infiles, outfile):
    """Concatenate each top_taxa_blastn.tsv per sample to create one file"""

    print(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S,%f')[:-3]} INFO main execution - creating erged_top_taxa_blastn.tsv for {epitope_name}", flush=True) 


    print(f"infiles: {infiles}")
    print(f"outfile: {outfile}")

    # create empty dataframe to store output
    merged_df = pd.DataFrame(
            columns=[
                "query",
                "total_hits",
                "subject_acc.ver",
                "top_taxa_hits",
                "alignment_length",
                "evalue",
                "bit_score",
                "perc_identity",
                "perc_query_coverage_per_subject",
                "subject_title",
            ]
        )
    
    # add data for each file
    for file in infiles:
        # define sampleid
        sample_id = re.search(rf"06_top_taxa_blastn.dir/{output_folder}/(.+)_top_taxa_blastn.tsv", str(file)).group(1)
        print(f"                                             concatenating dataframe for sample {sample_id}", flush=True)

        # open file as a dataframe
        file_df = pd.read_csv(
            file,
            sep="\t",
            comment="#",
            index_col=0
        )

        # concatenate files
        merged_df = pd.concat([merged_df, file_df])
    
    # save output
    merged_df.to_csv(outfile, sep="\t", na_rep='NULL')


###############################################################################
# Create final table by adding infered taxonomy to mimicry results
###############################################################################
@follows(merge_top_taxa)

@transform(
    f"06_top_taxa_blastn.dir/{output_folder}/merged_top_taxa_blastn.tsv",
    suffix(f"06_top_taxa_blastn.dir/{output_folder}/merged_top_taxa_blastn.tsv"),
    f"03_blast_search.dir/{output_folder}/{epitope_name}_epitope_blast_search_with_taxa_ids.csv"
)

def add_taxa(infile, outfile):
    """Find the best blastn taxonomy hit for each contig to the mimicry results"""

    # define file path to the mimicry blast search results
    mimicry_path = re.sub("_with_taxa_ids", "", outfile)
    mimicry_path = Path(re.sub("csv", "tsv", mimicry_path))

    # read in the blast mimicry search results as a dataframe
    mimicry_df = pd.read_csv(
        mimicry_path,
        sep="\t",
        comment="#",
        names=[
            "query_id",
            "subject_id",
            "perc_identity",
            "alignment_length",
            "mismatches",
            "gap opens",
            "q_start",
            "q_end",
            "s_start",
            "s_end",
            "evalue",
            "bit_score",
            "BTOP",
            "subject_seq",
            "subject_title",
            "subject_tax_id",
        ],
        dtype={
            "query_id" : str,
            "subject_id" : str,
            "perc_identity" : np.float64,
            "alignment_length" : np.float64,
            "mismatches" : np.float64,
            "gap opens" : np.float64,
            "q_start" : np.float64,
            "q_end" : np.float64,
            "s_start" : np.float64,
            "s_end" : np.float64,
            "evalue" : np.float64,
            "bit_score" : np.float64,
            "BTOP" : str,
            "subject_seq" : str,
            "subject_title" : str,
            "subject_tax_id" : str,
        }
    )

    # open dataframe containg top blast hit for each contig with potenital mimciry
    contig_df = pd.read_csv(
        infile,
        sep="\t",
        comment="#",
        na_values= "NULL",
        index_col=0
    )

    # select columns required
    contig_df = contig_df[[
            'query',
            'subject_acc.ver',
            'subject_title'
        ]]

    # rename columns
    contig_df = contig_df.rename(columns={
            'query': 'contig_query',
            'subject_acc.ver': 'top_taxa_ncbi_id',
            'subject_title': 'top_taxa_subject_title'
        }
    )

    # extract protein locus tag, and contig id
    contig_df[['subject_id','contig_id']] = contig_df['contig_query'].str.split("__", expand=True)

    # add contig taxonomy data to blast mimicry search output
    final_df = pd.merge(
        mimicry_df,
        contig_df,
        on = "subject_id",
        how="left"
    )
    
    # save output
    final_df.to_csv(outfile, na_rep='NULL')


###############################################################################
@follows(add_taxa)
def full():
    pass

def main(argv=None):
    if argv is None:
        argv = sys.argv
    P.main(argv)


if __name__ == "__main__":
    sys.exit(P.main(sys.argv))  